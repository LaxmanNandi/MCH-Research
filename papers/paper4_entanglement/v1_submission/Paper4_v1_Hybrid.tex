\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb}
\usepackage{mathptmx}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{natbib}
\usepackage{fancybox}
\usepackage{setspace}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=cyan,
    pdftitle={Engagement as Entanglement: Variance Signatures of Bidirectional Context Coupling in LLMs},
    pdfauthor={Laxman M M},
}

\title{\LARGE\textbf{Engagement as Entanglement: Variance Signatures of\\Bidirectional Context Coupling in Large Language Models}}

\author{
    \textbf{Dr.\ Laxman M M, MBBS}\\
    Government Duty Medical Officer, PHC Manchi\\
    Bantwal Taluk, Dakshina Kannada, Karnataka, India\\
    DNB General Medicine Resident (2026), KC General Hospital, Bangalore\\
    Email: \texttt{barlax5377@gmail.com}\\
    ORCID: \href{https://orcid.org/0009-0009-0405-6531}{0009-0009-0405-6531}
}

\date{February 2026}

\begin{document}

\maketitle

\begin{center}
\fbox{\parbox{0.9\textwidth}{
\textbf{Paper 4 of the MCH Research Program} --- This paper provides a mechanistic framework for the $\Delta$RCI metric introduced in Papers~1--2 and applied temporally in Paper~3. We demonstrate that context sensitivity tracks variance reduction in response embeddings, introducing a Variance Reduction Index (VRI) as a practical entanglement surrogate and identifying convergent versus divergent coupling as a deployment-relevant distinction.
}}
\end{center}

\vspace{0.5em}

% ============================================
% ABSTRACT
% ============================================
\begin{abstract}
\noindent Recent large-scale simulations demonstrate that LLMs exhibit systematic performance degradation in multi-turn conversations, with unreliability increasing 112\% across 200,000+ conversations \citep{laban2025lost}. However, this ``Lost in Conversation'' phenomenon lacks mechanistic explanation. We present an entanglement framework for understanding context sensitivity in large language models using embedding-level variance analysis across 12 model-domain runs (4 philosophy, 8 medical) and 360 position-level measurements. We demonstrate that $\Delta$RCI---a measure of context sensitivity introduced in Papers~1--2---tracks variance reduction in response embeddings. The correlation between $\Delta$RCI and the Variance Reduction Index ($\text{VRI} = 1 - \text{Var\_Ratio}$) is strong and highly significant ($r = 0.76$, $p = 8.2 \times 10^{-69}$, $N = 360$). This relationship reveals \emph{bidirectional} context coupling: convergent entanglement ($\text{Var\_Ratio} < 1$, $\Delta\text{RCI} > 0$), where context narrows the response distribution, and divergent entanglement ($\text{Var\_Ratio} > 1$, $\Delta\text{RCI} < 0$), where context widens it. The ``Lost in Conversation'' effect corresponds specifically to divergent entanglement. Two medical models (Llama 4 Scout: $\text{Var\_Ratio} = 7.46$; Llama 4 Maverick: $\text{Var\_Ratio} = 2.64$) exhibit extreme divergent entanglement at the summarization position (P30), producing highly unstable outputs when task enablement is expected. We introduce the Entanglement Stability Index (ESI) to predict which models will exhibit instability in multi-turn settings, transforming the descriptive observation that ``LLMs get lost'' into a predictive science of human-AI relational dynamics.

\vspace{0.5em}
\noindent\textbf{Keywords:} context sensitivity, $\Delta$RCI, entanglement, variance reduction, response predictability, LLM deployment safety, medical AI, multi-turn conversation
\end{abstract}

% ============================================
% 1. INTRODUCTION
% ============================================
\section{Introduction}

\subsection{The Problem: LLMs Get Lost in Conversation}

Laban et al.\ \citeyearpar{laban2025lost} conducted over 200,000 simulated conversations across 15 LLMs and documented systematic performance degradation in multi-turn settings: performance drops 39\%, with unreliability increasing by 112\%. Their central observation: ``When LLMs take a wrong turn in a conversation, they get lost and do not recover.''

This phenomenon is universal across tested models, from small open-weight systems (Llama-8B) to state-of-the-art architectures (GPT-4.1, Gemini 2.5 Pro). However, their work does not address \emph{why} unreliability increases, which models will fail, or whether multi-turn context is inherently detrimental.

\subsection{Our Contribution}

Context sensitivity in language models---the degree to which conversational history shapes responses---has been characterized across architectures and domains using $\Delta$RCI \citep{laxman2026paper1, laxman2026paper2} and shown to follow task-specific temporal patterns \citep{laxman2026paper3}. However, the \emph{mechanism} by which context shapes responses remains empirically uncharacterized.

$\Delta$RCI captures the aggregate change in response alignment with and without context, but does not describe how the \emph{distribution} of responses changes. A model with high $\Delta$RCI may achieve this through narrow, predictable outputs under context, or through a complex, high-variance coupling that pulls responses in consistent directions while increasing individual trial variance. These represent distinct deployment profiles.

This paper introduces an entanglement framework that connects $\Delta$RCI to the variance structure of response embeddings. The core insight is that context sensitivity can be understood as \textbf{predictability modulation}: context changes the shape of the response distribution, and $\Delta$RCI quantifies that change. When context narrows the distribution (\emph{convergent entanglement}), responses become more predictable. When context widens the distribution (\emph{divergent entanglement}), responses become less predictable---corresponding directly to the ``Lost in Conversation'' phenomenon.

We operationalize this framework using the Variance Reduction Index (VRI):
\begin{align}
    \text{VRI} &= 1 - \text{Var\_Ratio} \label{eq:vri}\\
    \text{Var\_Ratio} &= \frac{\text{Var}(\text{TRUE embeddings})}{\text{Var}(\text{COLD embeddings})} \label{eq:varratio}
\end{align}
where variance is computed across 50 independent trials at each conversational position. Positive VRI indicates that context reduces variance (convergent entanglement); negative VRI indicates that context increases variance (divergent entanglement).

The entanglement framework makes four contributions. First, it provides a mechanistic explanation for the ``Lost in Conversation'' phenomenon: divergent models exhibit $\text{Var\_Ratio} > 1$, producing unstable outputs as conversation length increases. Second, it reveals bidirectional coupling---context can either stabilize or destabilize responses---resolving the previously unexplained ``Sovereign'' category from Paper~1. Third, it identifies a concrete safety risk: models that exhibit extreme divergent entanglement at task-critical positions produce unpredictable outputs when predictability is most needed. Fourth, it introduces the Entanglement Stability Index (ESI) to predict which models will fail in multi-turn settings.

% ============================================
% 2. METHODS
% ============================================
\section{Methods}

\subsection{Data}

We analyze the 12-model subset from the MCH Research Program (Paper~2, v2 corrected dataset) that has complete response text preserved. Each model-domain run consists of 50 independent trials under three conditions (TRUE, COLD, SCRAMBLED) with 30 conversational prompts per trial. All runs used temperature = 0.7, max\_tokens = 1024, and all-MiniLM-L6-v2 embeddings (384-dimensional) \citep{reimers2019sentence}.

\subsection{Model Set}

\textbf{Philosophy (4 models, closed/commercial):}
GPT-4o, GPT-4o-mini, Claude Haiku, Gemini Flash.

\textbf{Medical (8 models):}
DeepSeek V3.1, Kimi K2, Llama 4 Maverick, Llama 4 Scout, Mistral Small 24B,
Ministral 14B, Qwen3 235B, Gemini Flash.

\noindent This 12-model subset represents a subset of the 25 model-domain runs from Paper~2; only runs with complete response text saved are included, as embedding-level variance computation requires access to raw responses.

\subsection{Metrics}

\textbf{$\Delta$RCI} was computed as in Papers~1--3: per-position $\text{mean}(\text{RCI}_{\text{TRUE}}) - \text{mean}(\text{RCI}_{\text{COLD}})$, where RCI is the mean pairwise cosine similarity within a condition across 50 trials.

\textbf{Note on RCI\textsubscript{COLD}:} RCI\textsubscript{COLD} reflects responses to prompts delivered with no conversational history (the COLD condition), not cross-condition similarity. Each RCI value is computed within its own condition.

\textbf{Var\_Ratio} was computed per position as $\text{Var}(\text{TRUE embeddings}) / \text{Var}(\text{COLD embeddings})$, where variance is the mean variance across all 384 embedding dimensions, computed across 50 independent trials at each prompt position.

\textbf{VRI} (Variance Reduction Index) $= 1 - \text{Var\_Ratio}$. Positive VRI indicates context reduces variance; negative VRI indicates context increases variance.

\textbf{ESI} (Entanglement Stability Index) $= 1 / |1 - \text{Var\_Ratio}|$. Models with ESI $< 1.0$ exhibit elevated instability risk; ESI $> 2.0$ indicates stable behavior.

\subsection{Statistical Analysis}

The primary test is the Pearson correlation between $\Delta$RCI and VRI across all 360 position-level measurements (12 models $\times$ 30 positions). Secondary analyses examine domain-specific variance patterns, model-level Var\_Ratio distributions, and the position-30 anomaly.

% ============================================
% 3. RESULTS
% ============================================
\section{Results}

\subsection{Finding 1: $\Delta$RCI tracks VRI (entanglement signal)}

Across 12 model-domain runs and 30 positions, $\Delta$RCI correlated strongly with VRI:

\begin{itemize}
    \item \textbf{Pooled correlation:} $r = 0.76$, $p = 8.2 \times 10^{-69}$ ($N = 360$ model-position points)
    \item Data: 12 model-domain runs $\times$ 30 positions $= 360$ points
    \item Each point aggregates 50 independent trials per condition
\end{itemize}

$\Delta$RCI increases as context reduces response variance. This supports the entanglement interpretation: context couples the response distribution to prior information, changing the predictability of outputs.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.80\textwidth]{../figures/entanglement_validation.png}
    \caption{\textbf{$\Delta$RCI vs VRI across 360 model-position points.}
    Blue: philosophy models (4). Red: medical models (8). The strong positive correlation ($r = 0.76$, $p = 8.2 \times 10^{-69}$) validates the entanglement framework: higher context sensitivity corresponds to greater variance reduction. Points in the lower-left quadrant represent divergent entanglement ($\text{Var\_Ratio} > 1$, $\Delta\text{RCI} < 0$), corresponding to the ``Lost in Conversation'' phenomenon documented by Laban et al.\ \citeyearpar{laban2025lost}.}
    \label{fig:entanglement}
\end{figure}

\noindent\textit{Note on scope:} Entanglement analysis requires actual response text to compute embedding variances. Only 12 of the 25 available model-domain runs from Paper~2 have complete response text preserved. Expansion to additional models would require rerunning experiments with response text preservation enabled.

\subsection{Finding 2: Bidirectional entanglement (convergent vs.\ divergent)}

The variance ratio reveals two distinct regimes:

\begin{itemize}
    \item \textbf{Convergent entanglement:} $\text{Var\_Ratio} < 1$, $\Delta\text{RCI} > 0$.
          Context narrows the response distribution, making outputs more predictable.
    \item \textbf{Divergent entanglement:} $\text{Var\_Ratio} > 1$, $\Delta\text{RCI} < 0$.
          Context widens the response distribution, making outputs less predictable.
\end{itemize}

This bidirectional framing resolves the ``Sovereign'' category from Paper~1: Sovereign behavior corresponds to divergent entanglement, where context destabilizes rather than stabilizes predictability. This is not a failure of context processing but a distinct mode of coupling.

Critically, divergent entanglement explains Laban et al.'s \citeyearpar{laban2025lost} findings: their +112\% unreliability increase corresponds to models entering divergent regime ($\text{Var\_Ratio} > 1$) as conversation length increases. Models do not ``get lost'' universally---only divergent models exhibit this pattern.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{../figures/fig4_entanglement_multipanel.png}
    \caption{\textbf{Multi-panel entanglement analysis.}
    The regime map shows convergent ($\text{Var\_Ratio} < 1$) and divergent ($\text{Var\_Ratio} > 1$) regions, with domain-specific patterns and position-dependent dynamics across medical and philosophy models.}
    \label{fig:multipanel}
\end{figure}

\subsection{Finding 3: Llama safety anomaly at medical P30}

At medical position 30 (the summarization prompt), two Llama models exhibit extreme divergent entanglement:

\begin{itemize}
    \item \textbf{Llama 4 Maverick:} $\text{Var\_Ratio} = 2.64$, $\Delta\text{RCI} = -0.15$, ESI $= 0.61$
    \item \textbf{Llama 4 Scout:} $\text{Var\_Ratio} = 7.46$, $\Delta\text{RCI} = -0.22$, ESI $= 0.15$
\end{itemize}

While other open medical models (Qwen3 235B, Mistral Small 24B) show mild divergence ($\text{Var\_Ratio} = 1.02$--$1.45$), only the Llama models exhibit extreme instability at P30. Convergent models at P30 show $\text{Var\_Ratio} < 1$ and positive $\Delta$RCI (Kimi K2, Ministral 14B, DeepSeek V3.1, Gemini Flash).

This identifies a \textbf{safety risk class}: models that diverge under closed-goal prompts produce highly unstable, unpredictable outputs precisely when task enablement is expected. The clinical implications---stochastically incomplete summaries with intact factual accuracy---are explored further in Paper~5 \citep{laxman2026paper5}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{../figures/fig7_llama_safety_anomaly.png}
    \caption{\textbf{Llama safety anomaly at medical P30.}
    Divergent variance signatures ($\text{Var\_Ratio} \gg 1$, ESI $< 1.0$) at the summarization position indicate extreme output instability in Llama 4 Maverick and Llama 4 Scout. Other open-weight medical models show mild or no divergence. Convergent models (Kimi K2, DeepSeek V3.1, Gemini Flash, Ministral 14B) show $\text{Var\_Ratio} < 1$ at P30, with ESI $> 2.0$ indicating stable multi-turn behavior.}
    \label{fig:llama}
\end{figure}

\subsection{Finding 4: Domain-specific variance patterns}

Mean variance ratios differ by domain:

\begin{itemize}
    \item \textbf{Philosophy:} $\text{Var\_Ratio} \approx 1.01$ (variance-neutral on average)
    \item \textbf{Medical:} $\text{Var\_Ratio} \approx 1.20$ (variance-increasing on average)
\end{itemize}

Medical prompts tend to destabilize response distributions under context, while philosophy is largely variance-neutral. This domain difference is consistent with the conservation constraint reported in Paper~6 \citep{laxman2026paper6}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.80\textwidth]{../figures/fig5_independence_rci_var.png}
    \caption{\textbf{RCI vs.\ Variance Ratio across models and positions.}
    Domain-specific relationship between context sensitivity and output variance. Medical models (red) cluster at higher Var\_Ratio than philosophy models (blue), indicating that closed-goal tasks systematically increase output variance under context.}
    \label{fig:independence}
\end{figure}

\subsection{Finding 5: ESI predicts multi-turn instability}

The Entanglement Stability Index provides a practical metric for predicting which models will exhibit the ``Lost in Conversation'' phenomenon:

\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Var\_Ratio (P30)} & \textbf{ESI} & \textbf{Assessment} \\
\midrule
Llama 4 Scout & 7.46 & 0.15 & Unstable \\
Llama 4 Maverick & 2.64 & 0.61 & Caution \\
Qwen3 235B & 1.02 & 50.0 & Stable \\
GPT-4o & 0.58 & 2.38 & Stable \\
Claude Haiku & 0.65 & 2.86 & Stable \\
Gemini Flash & 0.60 & 2.50 & Stable \\
\bottomrule
\end{tabular}
\caption{Entanglement Stability Index at medical P30. Models with ESI $< 1.0$ exhibit elevated risk of multi-turn degradation corresponding to Laban et al.'s \citeyearpar{laban2025lost} findings. ESI $> 2.0$ indicates stable multi-turn behavior.}
\end{table}

\subsection{Finding 6: Variance ratio as a practical entanglement surrogate}

The variance ratio provides a practical, low-cost surrogate for entanglement measurement. $\Delta$RCI tracks VRI ($r = 0.76$) without requiring $k$-NN entropy estimation or full mutual information computation. Computing Var\_Ratio requires only response embeddings and basic variance calculations, making entanglement measurement accessible at scale. This enables the deployment assessment framework developed in Paper~5.

% ============================================
% 4. DISCUSSION
% ============================================
\section{Discussion}

\subsection{Explaining the ``Lost in Conversation'' Phenomenon}

Laban et al.\ \citeyearpar{laban2025lost} documented that LLMs exhibit systematic unreliability increase (+112\%) in multi-turn conversations but did not provide mechanistic explanation. Our entanglement framework reveals that this phenomenon corresponds to \textbf{divergent entanglement}: models with $\text{Var\_Ratio} > 1$ produce increasingly variable outputs as conversation length increases.

Their observation that ``LLMs get lost and do not recover'' reflects self-reinforcing divergence: once a model enters the divergent regime, subsequent context compounds rather than constrains variance. Convergent models ($\text{Var\_Ratio} < 1$), by contrast, \emph{improve} with multi-turn context---contradicting the universal degradation narrative.

The ESI metric ($\text{ESI} = 1 / |1 - \text{Var\_Ratio}|$) provides predictive capability absent from their work: models with ESI $< 1.0$ are at elevated risk for multi-turn failure, while models with ESI $> 2.0$ exhibit stable behavior.

\subsection{Entanglement reframes $\Delta$RCI as predictability modulation}

The central conceptual shift is that $\Delta$RCI is not merely a measure of helpfulness but a \textbf{predictability modulation} measure. Context changes the shape of the response distribution; $\Delta$RCI quantifies that change. This reframing clarifies why negative $\Delta$RCI values are not inherently problematic: they indicate divergent entanglement, which may be useful for creative tasks but is concerning for safety-critical domains.

\subsection{Bidirectional entanglement resolves the Sovereign category}

The ``Sovereign'' category from Paper~1---models whose responses became less aligned under context---is now grounded in mechanism. Contexts that increase variance produce negative $\Delta$RCI. This is not a failure of context usage but a distinct mode of coupling. Divergent entanglement is a valid, measurable regime rather than a missing category.

\subsection{Safety implications: predictability is task-dependent}

The Llama divergence at medical P30 highlights a concrete risk: models can become less predictable exactly when a task presupposes context. For safety-critical tasks, \textbf{predictability is a requirement}, not an optional characteristic. Divergent entanglement in these settings should be treated as a deployment risk.

We propose (as provisional guidance) that $\text{Var\_Ratio} > 3.0$ or $\text{ESI} < 1.0$ at critical task positions be treated as a flag for further investigation. These thresholds are empirically motivated by the Llama anomaly but require validation against patient safety outcomes.

\subsection{Implications for AI evaluation}

Current LLM benchmarks emphasize zero-shot or few-shot performance, measuring base capability in isolation. Our findings demonstrate that multi-turn behavior constitutes a distinct evaluation dimension: models with equivalent zero-shot performance can exhibit radically different entanglement profiles (convergent vs.\ divergent).

We recommend that standardized evaluations report:
\begin{itemize}
    \item Var\_Ratio across conversation lengths
    \item ESI at task-critical positions
    \item Domain-specific entanglement signatures
\end{itemize}

This extends Laban et al.'s descriptive framework into actionable assessment criteria.

\subsection{Architectural interpretation}

The emergence of convergent versus divergent classes suggests differences in how models handle context saturation at task-critical positions. This observation is consistent across multiple models but remains \emph{descriptive} rather than causal. Future work should test whether divergence correlates with training objectives, attention patterns, or safety alignment strategies.

\subsection{Limitations}

\textbf{Model subset.} Only 12 of 25 model-domain runs have response text, limiting the analysis to 360 data points. Expansion requires rerunning experiments with text preservation.

\textbf{Embedding dependence.} Variance metrics depend on the all-MiniLM-L6-v2 embedding space. Alternative embedding models may yield different variance ratio distributions.

\textbf{Cross-model normalization.} Models with higher baseline variance (Var\_COLD) will naturally produce different Var\_Ratio magnitudes. We report raw values because absolute predictability change is deployment-relevant, but normalized comparisons would be appropriate for ranking models on relative sensitivity.

\textbf{Two domains.} This study analyzes text-only interactions in medical and philosophical domains only. Whether the convergent/divergent distinction generalizes to other domains (legal, creative, coding) requires further study.

\textbf{ESI threshold validation.} The $\text{ESI} < 1.0$ instability threshold is empirically derived from present data; validation across larger model sets and real-world deployment contexts is warranted.

\textbf{Observational scope.} Claims are matched to the experimental scope: text-only, two-domain, 50-trial evaluation with a focused model set. Broader generalizations are stated as hypotheses.

\subsection{Future Directions}

The entanglement framework presented here opens several research directions. First, \textbf{context utilization depth (CUD)} experiments currently in progress will test whether models with high ESI values can effectively integrate longer context windows, or whether engagement depth trades off against stability. Preliminary pilot data from 4 models suggest that Medical domain requires deeper context integration (K $>$ 15 messages) than Philosophy domain, consistent with the domain-specific entanglement signatures observed here.

Second, the framework should be tested across additional domains (legal reasoning, code generation, creative writing) to determine whether the engagement-as-entanglement interpretation generalizes beyond medical diagnostics and philosophical discourse. If the conservation constraint ΔRCI $\times$ Var\_Ratio $\approx$ K(domain) reported in Paper~6 holds across diverse task structures, it would suggest a fundamental trade-off in how language models allocate processing capacity.

Third, longitudinal studies tracking VRI and ESI across model versions could reveal whether architectural improvements enhance engagement capacity or merely redistribute existing capacity between context sensitivity and output stability. The distinction between convergent and divergent entanglement may reflect training objective choices (e.g., helpfulness vs.\ harmlessness tuning) rather than fundamental capability limits.

Finally, experimental validation of the ``Lost in Conversation'' recovery hypothesis is warranted: systematic testing of whether models that enter divergent regimes can be steered back to convergent behavior through intervention strategies (context summarization, retrieval augmentation, explicit instruction to focus). Supplementary Figure~S4 shows that 75\% of models in our dataset fail to recover after entering divergent states, but whether this reflects architectural constraints or merely default behavior patterns remains unknown.

% ============================================
% 5. CONCLUSION
% ============================================
\section{Conclusion}

Laban et al.\ \citeyearpar{laban2025lost} documented that LLMs ``get lost'' in multi-turn dialogue across 200,000+ conversations, with unreliability increasing 112\%. We provide mechanistic explanation through an entanglement framework: context sensitivity ($\Delta$RCI) tracks variance reduction in response embeddings (VRI), with a pooled correlation of $r = 0.76$ ($p = 8.2 \times 10^{-69}$, $N = 360$).

The framework reveals \textbf{bidirectional} coupling. Convergent entanglement narrows the response distribution ($\text{Var\_Ratio} < 1$), improving predictability with multi-turn context. Divergent entanglement widens the distribution ($\text{Var\_Ratio} > 1$), producing the ``Lost in Conversation'' phenomenon. The Entanglement Stability Index (ESI) predicts which models will fail: models with ESI $< 1.0$ exhibit elevated instability risk.

Two Llama models exhibit extreme divergent entanglement at the medical summarization position ($\text{Var\_Ratio}$ up to 7.46, ESI = 0.15), producing unstable outputs when task enablement is expected. This identifies a concrete safety risk class invisible to aggregate performance metrics but detectable with variance-based screening.

The Variance Reduction Index (VRI) provides a practical, low-cost surrogate for entanglement measurement, enabling the deployment predictability framework developed in Paper~5 \citep{laxman2026paper5} and the conservation constraint reported in Paper~6 \citep{laxman2026paper6}. Our framework transforms the descriptive observation that ``LLMs get lost'' into a predictive science of human-AI relational dynamics.

% ============================================
% ACKNOWLEDGMENTS
% ============================================
\section*{Acknowledgments}

This research builds on the MCH Research Program established in Paper~1. AI systems (Claude, ChatGPT, DeepSeek) assisted with data analysis, visualization, and manuscript preparation. The framework, findings, and interpretations remain the author's sole responsibility.

% ============================================
% DATA AVAILABILITY
% ============================================
\section*{Data Availability}

All raw data, response text, and analysis scripts are available in the project repository:
\url{https://github.com/LaxmanNandi/MCH-Research}

% ============================================
% SUPPLEMENTARY MATERIALS
% ============================================
\section*{Supplementary Materials}

Supplementary materials include:
\begin{itemize}
    \item \textbf{Figure S1}: Position-level ΔRCI and VRI correlation heatmaps
    \item \textbf{Figure S2}: Vendor-specific entanglement signatures
    \item \textbf{Figure S3}: ESI distribution across model classes
    \item \textbf{Figure S4}: ``Lost in Conversation'' experimental validation (6-panel analysis)
    \item \textbf{Table S1}: Complete model-position data (N=360)
    \item \textbf{Table S2}: ESI classification and recovery rates
\end{itemize}

All supplementary materials are available at: \url{https://github.com/LaxmanNandi/MCH-Research/tree/master/papers/paper4_entanglement/v1_submission/supplementary}

% ============================================
% REFERENCES
% ============================================
\bibliographystyle{plainnat}
\begin{thebibliography}{20}

\bibitem[Laban et al., 2025]{laban2025lost}
Laban, P., Hayashi, H., Zhou, Y., \& Neville, J.\ (2025).
\newblock LLMs get lost in multi-turn conversation.
\newblock {\em arXiv preprint}, arXiv:2505.06120.

\bibitem[Laxman, 2026a]{laxman2026paper1}
Laxman, M.\ M.\ (2026a).
\newblock Context curves behavior: Measuring AI relational dynamics with $\Delta$RCI.
\newblock {\em Preprints.org}, 202601.1881. DOI:~10.20944/preprints202601.1881.v2

\bibitem[Laxman, 2026b]{laxman2026paper2}
Laxman, M.\ M.\ (2026b).
\newblock Standardized context sensitivity benchmark across 25 LLM-domain configurations.
\newblock {\em Preprints.org}, 202602.1114. DOI:~10.20944/preprints202602.1114.v2

\bibitem[Laxman, 2026c]{laxman2026paper3}
Laxman, M.\ M.\ (2026c).
\newblock Domain-specific temporal dynamics of context sensitivity in large language models.
\newblock {\em Preprints.org}, ID:~199272.

\bibitem[Laxman, 2026d]{laxman2026paper5}
Laxman, M.\ M.\ (2026d).
\newblock Stochastic incompleteness in LLM summarization: A predictability taxonomy
for clinical AI deployment.
\newblock In preparation.

\bibitem[Laxman, 2026e]{laxman2026paper6}
Laxman, M.\ M.\ (2026e).
\newblock An empirical conservation constraint on context sensitivity and output variance:
Evidence across LLM architectures.
\newblock In preparation.

\bibitem[Brown et al., 2020]{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., et al.\ (2020).
\newblock Language models are few-shot learners.
\newblock {\em Advances in Neural Information Processing Systems}, 33.

\bibitem[Vaswani et al., 2017]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.\ N.,
Kaiser, \L., \& Polosukhin, I.\ (2017).
\newblock Attention is all you need.
\newblock {\em Advances in Neural Information Processing Systems}, 30.

\bibitem[Reimers \& Gurevych, 2019]{reimers2019sentence}
Reimers, N., \& Gurevych, I.\ (2019).
\newblock Sentence-BERT: Sentence embeddings using Siamese BERT-networks.
\newblock {\em Proceedings of EMNLP 2019}.

\bibitem[Asgari et al., 2025]{asgari2025framework}
Asgari, E., et al.\ (2025).
\newblock A framework to assess clinical safety and hallucination rates of LLMs
for medical text summarisation.
\newblock {\em npj Digital Medicine}, 8(1), 274.

\bibitem[Liu et al., 2024]{liu2024lost}
Liu, N.\ F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F.,
\& Liang, P.\ (2024).
\newblock Lost in the middle: How language models use long contexts.
\newblock {\em Transactions of the Association for Computational Linguistics}, 12, 104--123.

\end{thebibliography}

% ============================================
% SUPPLEMENTARY FIGURES
% ============================================
\clearpage
\appendix
\section*{Supplementary Figures}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{../figures/figure6_gaussian_verification.png}
    \caption{\textbf{Figure S1: Gaussian assumption verification.}
    Distribution analysis of $\Delta$RCI values across models and positions.
    Marginal normality is partially satisfied ($|\text{skew}| < 1$, $|\text{kurt}| < 3$
    for 61--76\% of dimensions), validating the use of Pearson correlation as the primary
    statistical test. Effective dimensionality: 12--23 of 384 dimensions.}
    \label{fig:gaussian}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{../figures/figure8_trial_convergence.png}
    \caption{\textbf{Figure S2: Trial-level convergence analysis.}
    $\Delta$RCI stability across 50 independent trials per model-domain run.
    Rolling 5-trial means show stable estimates throughout. Slope tests: 12/14 models
    show non-significant drift ($p > 0.05$), confirming 50-trial adequacy for stable estimation.}
    \label{fig:convergence}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{../figures/figure9_model_comparison.png}
    \caption{\textbf{Figure S3: Model-level $\Delta$RCI comparison.}
    Mean $\Delta$RCI values by model with 95\% confidence intervals.
    Medical models (closed-goal): mean $\Delta$RCI $= 0.341$;
    Philosophy models (open-goal): mean $\Delta$RCI $= 0.317$.
    Bar chart illustrates inter-model variability and domain separation.}
    \label{fig:model_comparison}
\end{figure}

\end{document}
